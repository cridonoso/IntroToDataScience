{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb63e98f",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "### Workshop 3\n",
    "\n",
    "*Jun 29 - IACS-MACI Internship*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0ea8a",
   "metadata": {},
   "source": [
    "Web scrapping is a computational techique for **automatically data extraction.**\n",
    "\n",
    "We can do scrapping in `pdf`, `docs`, and any other sources of information. The term `web` point to use HTML websites.\n",
    "\n",
    "There are **2 scrapping approaches** depeding on the website:\n",
    "- Static websites: HTML only \n",
    "- Dinamyc websites (also called applications): HTML + Javascript\n",
    "\n",
    "\n",
    "In Python we have more than one option to do scrapping.In the following table we show a comparison between the most popular libraries: \n",
    "\n",
    " <img src=\"images/scrap_python.png\" /> \n",
    "\n",
    "(*Source: [Python Web Scrapping - Kite, 2020](https://www.youtube.com/watch?v=zucvHSQsKHA)*)\n",
    "\n",
    "In this workshop we will use **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33095f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # to load html code\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd42a33",
   "metadata": {},
   "source": [
    "The first step is to load HTML code from some URL we want to explore.\n",
    "\n",
    "```\n",
    "https://arxiv.org/search/?query=machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=200\n",
    "````\n",
    "\n",
    "In this case, we use a list to save the keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d24267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/search/?query=machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=200\n"
     ]
    }
   ],
   "source": [
    "key_words = ['machine', 'learning']\n",
    "\n",
    "str_keywords = '+'.join(key_words)\n",
    "\n",
    "base = 'https://arxiv.org/search/?query={}&searchtype=all&abstracts=show&order=-announced_date_first&size=200'\n",
    "url  = base.format(str_keywords)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357ae42",
   "metadata": {},
   "source": [
    "Once we defined the url, then we need to open it on python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceed01df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.6 ms, sys: 2.38 ms, total: 40.9 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "html_page = requests.get(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f67d3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb9596",
   "metadata": {},
   "source": [
    "Using `BeatifulSoup` we can give an structure to the raw `HTML` code\n",
    "\n",
    "Notice we have [another formats](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser) such as `XML`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e008f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_page.text, 'html.parser') # Instanciamos nuestro scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e3439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b494e08",
   "metadata": {},
   "source": [
    "Now It's time to extract the information.\n",
    "\n",
    "The common way is using our browser in developer mode.\n",
    "\n",
    "1. Open the web site\n",
    "2. Right click on the item that we want to explore  <img src=\"./images/ws_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "3. Click on Inspect Element <img src=\"./images/ws_2.png\" alt=\"Drawing\" style=\"width: 200px;\"/> \n",
    "4. Look at the HTML tags to access from our scraper <img src=\"./images/ws_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f04546",
   "metadata": {},
   "source": [
    "In this example we are going to get the **titles** of the articles. Thus, we need to extract paragraphs ```<p>``` whose class is named ```class=\"title is-5 mathjax\">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59504b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('p', attrs = {\"class\": 'title is-5 mathjax'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51b1e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning a Single Neuron with Adversarial Label Noise via Gradient Descent\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(titulo.text.strip())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1dcfc",
   "metadata": {},
   "source": [
    "You can also access to the object attributes. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b76fde3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': ['title', 'is-5', 'mathjax']}\n",
      "['title', 'is-5', 'mathjax']\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(titulo.attrs)\n",
    "    class_value = titulo.attrs.get('class')\n",
    "    print(class_value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40279c4",
   "metadata": {},
   "source": [
    "## Hands on code!\n",
    "\n",
    "Create a DataFrame containng the title of the paper and the url to it.\n",
    "\n",
    "**Hint**: All the papers are listed in `<ol>` an ordered list. Thus every article block is into the `<li>` tags. You have to iterate over those objects capturing the required information\n",
    "\n",
    "**Hint 2**: You can do as many queries as tags in your html code. Go from the most general to the most specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acfb6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_elements = **your code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_column  = [] # to save titles\n",
    "link_column = [] # to save links\n",
    "\n",
    "for item in list_elements:\n",
    "    title = soup.find_all('p', attrs = {\"class\": 'title is-5 mathjax'})\n",
    "    link_content = *your code here*\n",
    "    link_content = *your code here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f2060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
